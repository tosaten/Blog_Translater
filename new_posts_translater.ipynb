{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import json\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pyperclip\n",
    "import schedule\n",
    "import datetime\n",
    "\n",
    "\n",
    "def new_posts_send_discord(main_url, post_list_class, post_url_class):\n",
    "    try_count = 0\n",
    "\n",
    "    #--------new posts scraping--------\n",
    "\n",
    "    site = requests.get(main_url)\n",
    "    soup = BeautifulSoup(site.text,'html.parser')\n",
    "\n",
    "    #postsに最新投稿のデータを格納\n",
    "    posts = soup.find(class_=post_class_).find_all(post_tag)\n",
    "\n",
    "    #空リストの作成\n",
    "    articles = []\n",
    "    new_posts_url_list = []\n",
    "\n",
    "    try_count = 0\n",
    "    if try_count == 0:\n",
    "        befor_posts = [] #１度目の試行のみ\n",
    "        try_count += 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # 前回と今回のスクレイピング結果を比較して、新しいものだけリストで返す\n",
    "    def search_post(posts, befor_posts):\n",
    "        for post in posts:\n",
    "            #前回取得したリストに入ってない投稿があれば\n",
    "            if post not in befor_posts:\n",
    "                #articlesに格納\n",
    "                articles.append(post)\n",
    "        return articles\n",
    "\n",
    "    new_posts = search_post(posts, befor_posts)\n",
    "    for x in new_posts:\n",
    "        #new_posts_url = x.find('div',class_ = \"col-md-3 post-thumb\").find('a').get('href')\n",
    "        new_posts_url = x.post_list_tag.find(\"a\").get(\"href\")\n",
    "        new_posts_url_list.append(new_posts_url)\n",
    "\n",
    "    befor_posts = new_posts_url_list\n",
    "\n",
    "    #--------取得した最新記事を投稿できる形に変換--------\n",
    "    #もし最新記事があったら\n",
    "    if len(new_posts_url_list) >= 1:\n",
    "\n",
    "        print(str(len(new_posts_url_list)) + \"件の最新記事を見つけました\")\n",
    "\n",
    "        for url in new_posts_url_list:\n",
    "\n",
    "            site = requests.get(url)\n",
    "            soup = BeautifulSoup(site.text,'html.parser')\n",
    "            [tag.extract() for tag in soup(string='n')]\n",
    "\n",
    "            #記事のタイトルの取得\n",
    "            title = soup.find('h1').text\n",
    "\n",
    "            #記事の文章の取得\n",
    "            a = soup.find('div', class_ = 'entry-content col-md-10')\n",
    "            body_list = []\n",
    "\n",
    "            #コンテンツがインデックスエラーを起こすまで試行\n",
    "            #もしそのリストのタグが規定のものであれば，body_listに格納\n",
    "            stop = 0\n",
    "            n = 0\n",
    "            back_h3 = \"\\n\\n**\"\n",
    "            front_h3 = \"**\\n\"\n",
    "\n",
    "            #取得した本文を本文リストに追加\n",
    "            while stop <= 1:\n",
    "                try:\n",
    "                    #<div>タグが現れたらストップ\n",
    "                    if a.contents[n].name == 'div':\n",
    "                        stop += 1\n",
    "                    #見出しタグが現れたら，改行と太字にする\n",
    "                    elif a.contents[n].name == 'h3':\n",
    "                        b = a.contents[n].text\n",
    "                        body_list.append(back_h3)\n",
    "                        body_list.append(b)\n",
    "                        body_list.append(front_h3)\n",
    "                    else:\n",
    "                        b = a.contents[n].text\n",
    "                        body_list.append(b)\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                n += 1\n",
    "        return body_list\n",
    "        #なかったら\n",
    "    else:\n",
    "        return ('最新記事はありませんでした')\n",
    "        \n",
    "\n",
    "\n",
    "#-----------selenium操作-----------\n",
    "\n",
    "transfer_body_list = []\n",
    "\n",
    "#ChromeDriverのパスを引数に指定しChromeを起動\n",
    "driver = webdriver.Chrome(\"../blog/chromedriver.exe\")\n",
    "#指定したURLに遷移する\n",
    "driver.get(\"https://www.deepl.com/ja/translator\")\n",
    "time.sleep(3)\n",
    "\n",
    "#検索テキストボックスの要素をId属性名から取得\n",
    "text_box = driver.find_element_by_xpath(\"/html/body/div[2]/div[1]/div[5]/div[2]/div[1]/div[2]/div/textarea\")\n",
    "input_selector = driver.find_element_by_css_selector(\"#dl_translator > div.lmt__text > div.lmt__sides_container > div.lmt__side_container.lmt__side_container--target > div.lmt__textarea_container > div.lmt__target_toolbar.lmt__target_toolbar--visible > div.lmt__target_toolbar__copy > button\")\n",
    "#output_box = element.find_element_by_xpath('/html/body/div[2]/div[1]/div[5]/div[2]/div[3]/div[3]/div[6]/div[2]/button')\n",
    "\n",
    "#検索テキストボックスに文字列を入力\n",
    "text_box.send_keys(title)\n",
    "#翻訳されるまで待つ\n",
    "time.sleep(5)\n",
    "#クリップボックスに出力をコピー\n",
    "input_selector.click()\n",
    "time.sleep(1)\n",
    "#クリップボードから翻訳した文字列を変数に格納\n",
    "transfer_title = pyperclip.paste()\n",
    "\n",
    "text_box.clear()\n",
    "\n",
    "for befor_body_list in body_list:\n",
    "    #検索テキストボックスに文字列を入力\n",
    "    text_box.send_keys(befor_body_list)\n",
    "    #翻訳されるまで待つ\n",
    "    time.sleep(3)\n",
    "\n",
    "    # this scrolls untill the element is in the middle of the page\n",
    "    element = input_selector\n",
    "    desired_y = (element.size['height'] / 2) + element.location['y']\n",
    "    current_y = (driver.execute_script('return window.innerHeight') / 2) + driver.execute_script ('return window.pageYOffset')\n",
    "    scroll_y_by = desired_y - current_y\n",
    "    driver.execute_script(\"window.scrollBy(0, arguments[0]);\", scroll_y_by)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    input_selector.click()\n",
    "\n",
    "    time.sleep(0.5)\n",
    "    #クリップボードから翻訳した文字列を変数に格納\n",
    "    paste_str = pyperclip.paste()\n",
    "    transfer_body_list.append(paste_str)\n",
    "    \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    text_box.clear()\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "    #なかったら\n",
    "    else:\n",
    "        print('最新記事はありませんでした')"
   ]
  }
 ]
}